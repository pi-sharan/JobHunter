{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = pd.read_csv('../input_data/apps.tsv', delimiter='\\t',encoding='utf-8')\n",
    "user_history = pd.read_csv('../input_data/user_history.tsv', delimiter='\\t',encoding='utf-8')\n",
    "jobs = pd.read_csv('../input_data/jobs.tsv', delimiter='\\t',encoding='utf-8', on_bad_lines=\"skip\")\n",
    "users = pd.read_csv('../input_data/users.tsv' ,delimiter='\\t',encoding='utf-8')\n",
    "test_users = pd.read_csv('../input_data/test_users.tsv', delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Window = 6**\n",
    "\n",
    "In our project, we are using only a subset of the original dataset, due to its large nature. We will only be using the data from one of the windows (here window 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>WindowID</th>\n",
       "      <th>Split</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>JobTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1337041</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>1</td>\n",
       "      <td>Pennsylvania Mentor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337042</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>2</td>\n",
       "      <td>Student Worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337043</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>3</td>\n",
       "      <td>Internship in Adoption Unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337044</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337045</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "      <td>Student Worker - Continuing Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530889</th>\n",
       "      <td>1472087</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>3</td>\n",
       "      <td>GloBull Ambassador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530890</th>\n",
       "      <td>1472087</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "      <td>Research Assistant for Head Start Study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530891</th>\n",
       "      <td>1472087</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "      <td>Volunteer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530892</th>\n",
       "      <td>1472087</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>6</td>\n",
       "      <td>Customer Service Associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530893</th>\n",
       "      <td>1472087</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>7</td>\n",
       "      <td>Customer Service Lead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193853 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          UserID  WindowID  Split  Sequence  \\\n",
       "1337041       13         6   Test         1   \n",
       "1337042       13         6   Test         2   \n",
       "1337043       13         6   Test         3   \n",
       "1337044       13         6   Test         4   \n",
       "1337045       13         6   Test         5   \n",
       "...          ...       ...    ...       ...   \n",
       "1530889  1472087         6  Train         3   \n",
       "1530890  1472087         6  Train         4   \n",
       "1530891  1472087         6  Train         5   \n",
       "1530892  1472087         6  Train         6   \n",
       "1530893  1472087         6  Train         7   \n",
       "\n",
       "                                        JobTitle  \n",
       "1337041                      Pennsylvania Mentor  \n",
       "1337042                           Student Worker  \n",
       "1337043              Internship in Adoption Unit  \n",
       "1337044                                      NaN  \n",
       "1337045    Student Worker - Continuing Education  \n",
       "...                                          ...  \n",
       "1530889                       GloBull Ambassador  \n",
       "1530890  Research Assistant for Head Start Study  \n",
       "1530891                                Volunteer  \n",
       "1530892               Customer Service Associate  \n",
       "1530893                    Customer Service Lead  \n",
       "\n",
       "[193853 rows x 5 columns]"
      ]
     },
     "execution_count": 55,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_history[user_history.WindowID==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 115998 entries, 861371 to 977368\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   JobID         115998 non-null  int64 \n",
      " 1   WindowID      115998 non-null  int64 \n",
      " 2   Title         115996 non-null  object\n",
      " 3   Description   115997 non-null  object\n",
      " 4   Requirements  115923 non-null  object\n",
      " 5   City          115998 non-null  object\n",
      " 6   State         115998 non-null  object\n",
      " 7   Country       115998 non-null  object\n",
      " 8   Zip5          71528 non-null   object\n",
      " 9   StartDate     115998 non-null  object\n",
      " 10  EndDate       115998 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 10.6+ MB\n"
     ]
    }
   ],
   "source": [
    "jobs[jobs.WindowID==6].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 43334 entries, 296639 to 339972\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   UserID                43334 non-null  int64  \n",
      " 1   WindowID              43334 non-null  int64  \n",
      " 2   Split                 43334 non-null  object \n",
      " 3   City                  43334 non-null  object \n",
      " 4   State                 43276 non-null  object \n",
      " 5   Country               43334 non-null  object \n",
      " 6   ZipCode               43142 non-null  object \n",
      " 7   DegreeType            32034 non-null  object \n",
      " 8   Major                 32428 non-null  object \n",
      " 9   GraduationDate        29703 non-null  object \n",
      " 10  WorkHistoryCount      43334 non-null  int64  \n",
      " 11  TotalYearsExperience  41733 non-null  float64\n",
      " 12  CurrentlyEmployed     40653 non-null  object \n",
      " 13  ManagedOthers         43334 non-null  object \n",
      " 14  ManagedHowMany        43334 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(10)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "users[users.WindowID==6].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26006 entries, 296639 to 339971\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   UserID                26006 non-null  int64  \n",
      " 1   WindowID              26006 non-null  int64  \n",
      " 2   Split                 26006 non-null  object \n",
      " 3   City                  26006 non-null  object \n",
      " 4   State                 26006 non-null  object \n",
      " 5   Country               26006 non-null  object \n",
      " 6   ZipCode               25971 non-null  object \n",
      " 7   DegreeType            26006 non-null  object \n",
      " 8   Major                 26006 non-null  object \n",
      " 9   GraduationDate        20566 non-null  object \n",
      " 10  WorkHistoryCount      26006 non-null  int64  \n",
      " 11  TotalYearsExperience  26006 non-null  float64\n",
      " 12  CurrentlyEmployed     26006 non-null  object \n",
      " 13  ManagedOthers         26006 non-null  object \n",
      " 14  ManagedHowMany        26006 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(10)\n",
      "memory usage: 3.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Split\n",
       " Train    25021\n",
       " Test       985\n",
       " Name: count, dtype: int64,\n",
       " None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_set = users[(users.WindowID==6) & (users.Country==\"US\")].dropna(axis=0,subset=[\"Major\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \"DegreeType\"])\n",
    "user_set.Split.value_counts(), user_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the preprocess.ipynb file, the number of US apps are very high. We will only be considering US applications.<br>\n",
    "Furthermore, we willbe dropping all such users, who do not have a user_history attached to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 115684 entries, 861371 to 977368\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   JobID         115684 non-null  int64 \n",
      " 1   WindowID      115684 non-null  int64 \n",
      " 2   Title         115684 non-null  object\n",
      " 3   Description   115684 non-null  object\n",
      " 4   Requirements  115684 non-null  object\n",
      " 5   City          115684 non-null  object\n",
      " 6   State         115684 non-null  object\n",
      " 7   Country       115684 non-null  object\n",
      " 8   Zip5          71502 non-null   object\n",
      " 9   StartDate     115684 non-null  object\n",
      " 10  EndDate       115684 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 10.6+ MB\n"
     ]
    }
   ],
   "source": [
    "job_set = jobs[(jobs.WindowID==6) & (jobs.Country==\"US\")].dropna(axis=0,subset=[\"Description\",\"Requirements\",\"Title\"])\n",
    "job_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         UserID  WindowID  Split          ApplicationDate   JobID\n",
      "1247132      13         6   Test  2012-06-19 15:36:38.583  821691\n",
      "1247136      64         6  Train  2012-06-06 14:32:43.753  666073\n",
      "1247137      64         6  Train  2012-06-06 14:18:55.773  281940\n",
      "1247138      64         6  Train  2012-06-06 14:40:26.137  337025\n",
      "1247139     101         6  Train  2012-06-06 11:47:59.313  949251\n"
     ]
    }
   ],
   "source": [
    "user_id = user_set.UserID.unique().tolist()\n",
    "job_id = job_set.JobID.unique().tolist()\n",
    "# get work history of users present in the user_set\n",
    "work_history = user_history[user_history.UserID.isin(user_id)]\n",
    "work_history.dropna(axis=0,subset=[\"JobTitle\"], inplace=True)\n",
    "\n",
    "# filtering job applications data to only get applications of users and jobs present in the user_set and job_set\n",
    "application_record = apps[(apps.UserID.isin(user_id))&(apps.JobID.isin(job_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further reducing the user and job set to unique values \n",
    "work_user_id = work_history.UserID.unique()\n",
    "application_user_id = application_record.UserID.unique()\n",
    "user_set = user_set[(user_set.UserID.isin(work_user_id))&(user_set.UserID.isin(application_user_id))]\n",
    "user_id = user_set.UserID.unique()\n",
    "application_record = application_record[application_record.UserID.isin(user_id)]\n",
    "work_history = work_history[work_history.UserID.isin(user_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set.to_csv(\"users.csv\", index=False)\n",
    "application_record.to_csv(\"application_record.csv\", index=False)\n",
    "work_history.to_csv(\"work_history.csv\", index=False)\n",
    "job_set.to_csv(\"jobs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative-sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x11fd38210>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/22133 [00:00<01:26, 254.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         UserID  WindowID Split          ApplicationDate   JobID\n",
      "1247132      13         6  Test  2012-06-19 15:36:38.583  821691\n",
      "         UserID  WindowID  Split          ApplicationDate   JobID\n",
      "1247136      64         6  Train  2012-06-06 14:32:43.753  666073\n",
      "1247137      64         6  Train  2012-06-06 14:18:55.773  281940\n",
      "1247138      64         6  Train  2012-06-06 14:40:26.137  337025\n",
      "         UserID  WindowID  Split          ApplicationDate   JobID\n",
      "1247139     101         6  Train  2012-06-06 11:47:59.313  949251\n",
      "         UserID  WindowID  Split          ApplicationDate  JobID\n",
      "1247140     133         6  Train  2012-06-05 11:33:45.903  17494\n",
      "         UserID  WindowID  Split          ApplicationDate    JobID\n",
      "1247150     182         6  Train  2012-06-11 00:21:47.237  1098447\n",
      "1247151     182         6  Train  2012-06-06 18:34:33.303   428902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#all the jobs the user has applied to grouped by user-id\n",
    "final_apps = pd.DataFrame(columns = [\"UserID\",\"JobID\",\"label\"])\n",
    "job_id = job_set.JobID.unique().tolist()\n",
    "groups = application_record.groupby(\"UserID\")\n",
    "user_ids = []\n",
    "job_ids = []\n",
    "labels = []\n",
    "\n",
    "print(groups)\n",
    "i=0\n",
    "for id, group in tqdm(groups):\n",
    "    print(group)\n",
    "    # print()\n",
    "    # print()\n",
    "    # print()\n",
    "    # print(id)\n",
    "    i=i+1\n",
    "    if i==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22133/22133 [01:20<00:00, 275.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, group in tqdm(groups):\n",
    "    size = len(group)\n",
    "    exist_job = group.JobID.unique().tolist()\n",
    "    candidate_job = [i for i in job_id if i not in exist_job ]\n",
    "    sample_job = np.random.randint(0,len(candidate_job),size)\n",
    "    user_ids.extend([idx] * 2 * size)\n",
    "    exist_job.extend([candidate_job[i] for i in sample_job])\n",
    "    job_ids.extend(exist_job)\n",
    "    label = [1] * size\n",
    "    label.extend([0] * size)\n",
    "    labels.extend(label)\n",
    "\n",
    "final_apps.UserID = user_ids\n",
    "final_apps.JobID = job_ids\n",
    "final_apps.label = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    103657\n",
       "0    103657\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_apps.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_apps.to_csv(\"apps.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning Jobs Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.read_csv(\"jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "removePattern = r'(<(.*?)>)|(&\\w+)'\n",
    "addSpacePattern = r'([;:])|(\\\\r)|(\\\\n)'\n",
    "removeExtraSpaces = r'(\\s\\s+?)(?=\\S)'\n",
    "\n",
    "jobs['DescCleaned'] = jobs['Description'].astype(str).str.lower()\n",
    "jobs['DescCleaned'] = jobs['DescCleaned'].apply(lambda x: re.sub(removePattern, \"\", x))\n",
    "jobs['DescCleaned'] = jobs['DescCleaned'].apply(lambda x: re.sub(addSpacePattern, \" \", x))\n",
    "jobs['DescCleaned'] = jobs['DescCleaned'].apply(lambda x: re.sub(removeExtraSpaces, \" \", x))\n",
    "# Similarly for requirements\n",
    "jobs['ReqCleaned'] = jobs['Requirements'].astype(str).str.lower()\n",
    "jobs['ReqCleaned'] = jobs['ReqCleaned'].apply(lambda x: re.sub(removePattern, \"\", x))\n",
    "jobs['ReqCleaned'] = jobs['ReqCleaned'].apply(lambda x: re.sub(addSpacePattern, \" \", x))\n",
    "jobs['ReqCleaned'] = jobs['ReqCleaned'].apply(lambda x: re.sub(removeExtraSpaces, \" \", x))\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.drop(columns=['Description', 'Requirements'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.to_csv(\"jobs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = pd.read_csv('apps.csv')\n",
    "jobs = pd.read_csv('jobs.csv')\n",
    "users = pd.read_csv('users.csv')\n",
    "work_history = pd.read_csv('work_history.csv')"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobs.fillna(\" \")\n",
    "jobs[\"word\"] = jobs.Title + jobs.DescCleaned + jobs.ReqCleaned\n",
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=5, max_features=100, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(jobs['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRIAL 1**\n",
    "\n",
    "Here, I am removing all users who have more than 20 applications."
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2320"
      ]
     },

     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sorted(dict(apps.UserID.value_counts()).items(), key=lambda x: x[1], reverse=True)\n",
    "exclude_user_id = [i[0] for i in temp if i [1]>=20]\n",
    "len(exclude_user_id)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = apps[~apps.UserID.isin(exclude_user_id)]"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = apps.UserID.unique()\n",
    "work_history = work_history[work_history.UserID.isin(user_id)]\n",
    "users = users[users.UserID.isin(user_id)]\n",
    "users.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should drop any duplicates in work_history (I saw a few of them)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_history = work_history.drop(columns=[\"Sequence\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>WindowID</th>\n",
       "      <th>Split</th>\n",
       "      <th>JobTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>Pennsylvania Mentor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>Student Worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>Internship in Adoption Unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>Student Worker - Continuing Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>Sales Associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97339</th>\n",
       "      <td>1471948</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>Assistant (P/T)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97340</th>\n",
       "      <td>1471948</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>Phone Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97341</th>\n",
       "      <td>1472019</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>Supply Admin Clerk/ Combat Marksmanship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97342</th>\n",
       "      <td>1472066</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97343</th>\n",
       "      <td>1472066</td>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>Team Lead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80542 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        UserID  WindowID  Split                                 JobTitle\n",
       "0           13         6   Test                      Pennsylvania Mentor\n",
       "1           13         6   Test                           Student Worker\n",
       "2           13         6   Test              Internship in Adoption Unit\n",
       "3           13         6   Test    Student Worker - Continuing Education\n",
       "4           13         6   Test                          Sales Associate\n",
       "...        ...       ...    ...                                      ...\n",
       "97339  1471948         6  Train                          Assistant (P/T)\n",
       "97340  1471948         6  Train                              Phone Sales\n",
       "97341  1472019         6  Train  Supply Admin Clerk/ Combat Marksmanship\n",
       "97342  1472066         6  Train                                  Manager\n",
       "97343  1472066         6  Train                                Team Lead\n",
       "\n",
       "[80542 rows x 4 columns]"
      ]
     },

     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_history"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_with_space(series):\n",
    "    return ' '.join(series)\n",
    "\n",
    "work_history_tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0.0, max_features=50, stop_words='english')\n",
    "work_history_tf_matrix = work_history_tf.fit_transform(work_history.groupby('UserID')['JobTitle'].agg(sum_with_space).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean the users and jobs dataset**"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.drop(columns=[\"Country\",\"ZipCode\",\"Major\",\"GraduationDate\",\"WindowID\"])"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.replace({\"CurrentlyEmployed\":{\"Yes\":1,\"No\":0}}, inplace=True)\n",
    "users.replace({\"ManagedOthers\":{\"Yes\":1,\"No\":0}}, inplace=True)\n",
    "users.replace({\"DegreeType\":{\"None\":0,\"High School\":1, \"Vocational\":2, \"Associate's\":3, \"Bachelor's\":4, \"Master's\":5, \"PhD\":6}}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_city_dict = dict(zip(users['UserID'], users['City']))\n",
    "user_state_dict = dict(zip(users['UserID'], users['State']))\n",
    "\n",
    "jobs_city_dict = dict(zip(jobs['JobID'], jobs['City']))\n",
    "jobs_state_dict = dict(zip(jobs['JobID'], jobs['State']))"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "100%|██████████| 107176/107176 [00:01<00:00, 87845.28it/s]\n"

     ]
    }
   ],
   "source": [
    "city = []\n",
    "state = []\n",
    "for index, row in tqdm(apps.iterrows(), total=len(apps)):\n",
    "    city.append(1 if jobs_city_dict[row['JobID']] == user_city_dict[row['UserID']] else 0)\n",
    "    state.append(1 if jobs_state_dict[row['JobID']] == user_state_dict[row['UserID']] else 0)\n",
    "\n",
    "apps[\"City\"] = city\n",
    "apps[\"State\"] = state"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = users[users.Split==\"Train\"].UserID.values\n",
    "test_user = users[users.Split==\"Test\"].UserID.values\n",
    "train_data = apps[apps.UserID.isin(train_user)]\n",
    "test_data = apps[apps.UserID.isin(test_user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 104700 entries, 2 to 207313\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   UserID  104700 non-null  int64\n",
      " 1   JobID   104700 non-null  int64\n",
      " 2   label   104700 non-null  int64\n",
      " 3   City    104700 non-null  int64\n",
      " 4   State   104700 non-null  int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 4.8 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "100%|██████████| 19473/19473 [03:13<00:00, 100.62it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = train_data.groupby(\"UserID\")\n",
    "X_train = np.zeros((1,158))\n",
    "Y_train = []\n",
    "for u_id, group in tqdm(groups):\n",
    "    # print(u_id)\n",
    "    user = users[users.UserID==u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                            \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "    u_idx = user.index.values[0]\n",
    "\n",
    "    # print(u_idx)\n",
    "    \n",
    "    user_feature = np.concatenate((user.values, work_history_tf_matrix[u_idx,:].toarray()),axis=1)\n",
    "    job_id_list = group.JobID.values   #all the jobs the user has applied / not applied to \n",
    "    temp_jobs = jobs[jobs.JobID.isin(job_id_list)]  # from all jobs, get the jobs from the job_id_list\n",
    "    j_idx = temp_jobs.index.values\n",
    "    f = []\n",
    "    for i in j_idx:\n",
    "        feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "        feature = np.concatenate((feature, ))\n",
    "        f.append(feature)\n",
    "    # print(len(group[[\"City\",\"State\"]].values), ' ', len(temp_jobs))\n",
    "    feature = np.concatenate((group[[\"City\",\"State\"]].values, np.array(f)),axis=1)\n",
    "    X_train = np.concatenate((X_train, feature), axis=0)\n",
    "    Y_train.extend(group.label.values.tolist())\n",
    "X_train = X_train[1:]"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104700, 158), 104700)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, len(Y_train)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "100%|██████████| 340/340 [00:00<00:00, 486.26it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = test_data.groupby(\"UserID\")\n",
    "X_test = np.zeros((1,158))\n",
    "Y_test = []\n",
    "for u_id, group in tqdm(groups):\n",
    "    user = users[users.UserID==u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                            \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "    u_idx = user.index.values[0]\n",
    "\n",
    "    user_feature = np.concatenate((user.values, work_history_tf_matrix[u_idx,:].toarray()),axis=1) # UI\n",
    "\n",
    "    job_id_list = group.JobID.values\n",
    "    temp_jobs = jobs[jobs.JobID.isin(job_id_list)] # job table\n",
    "    \n",
    "    j_idx = temp_jobs.index.values\n",
    "    f = []\n",
    "    for i in j_idx:\n",
    "        feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "        f.append(feature)\n",
    "    feature = np.concatenate((group[[\"City\",\"State\"]].values, np.array(f)),axis=1)\n",
    "    X_test = np.concatenate((X_test, feature), axis=0)\n",
    "    Y_test.extend(group.label.values.tolist())\n",
    "X_test = X_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 4.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 4.        , ..., 0.14426976, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 1.        , ..., 0.16595852, 0.        ,\n",
       "        0.12169042],\n",
       "       ...,\n",
       "       [0.        , 0.        , 4.        , ..., 0.09134875, 0.        ,\n",
       "        0.05023666],\n",
       "       [0.        , 0.        , 4.        , ..., 0.48112811, 0.07308761,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 4.        , ..., 0.17415049, 0.25132248,\n",
       "        0.19154588]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(y_true, y_prediction):\n",
    "    report = classification_report(y_true,y_prediction,digits=4)\n",
    "    report = report.splitlines()\n",
    "    columns = ['class'] + report[0].split()\n",
    "    col_1, col_2, col_3, col_4, col_5 = [], [], [], [], []\n",
    "    for row in report[1:]:\n",
    "        if len(row.split()) != 0:\n",
    "            row = row.split()\n",
    "            if len(row) < 5:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append('')\n",
    "                col_3.append('')\n",
    "                col_4.append(row[1])\n",
    "                col_5.append(row[2])\n",
    "            elif len(row) > 5:\n",
    "                col_1.append(row[0] + ' ' + row[1])\n",
    "                col_2.append(row[2])\n",
    "                col_3.append(row[3])\n",
    "                col_4.append(row[4])\n",
    "                col_5.append(row[5])\n",
    "            else:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append(row[1])\n",
    "                col_3.append(row[2])\n",
    "                col_4.append(row[3])\n",
    "                col_5.append(row[4])\n",
    "    col_1.append(\"overall\")\n",
    "    col_2.append(precision_score(y_true, y_prediction))\n",
    "    col_3.append(recall_score(y_true, y_prediction))\n",
    "    col_4.append(f1_score(y_true, y_prediction))\n",
    "    col_5.append(roc_auc_score(y_true, y_prediction))\n",
    "    result = pd.DataFrame()\n",
    "    result[columns[0]] = col_1\n",
    "    result[columns[1]] = col_2\n",
    "    result[columns[2]] = col_3\n",
    "    result[columns[3]] = col_4\n",
    "    result[columns[4]] = col_5\n",
    "    print(\"——————Test——————\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.8614    0.9588    0.9075      1238\n",
      "1             1    0.9536    0.8457    0.8964      1238\n",
      "2      accuracy                        0.9023      2476\n",
      "3     macro avg    0.9075    0.9023    0.9019      2476\n",
      "4  weighted avg    0.9075    0.9023    0.9019      2476\n",
      "5       overall  0.953552  0.845719  0.896404  0.902262\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred = [0 if i<0.5 else 1 for i in y_pred]\n",
    "show_result(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert X_train and Y_train to numpy arrays if they are not already\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "# Similarly, convert X_test and Y_test if needed\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 4.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 4.        , ..., 0.14426976, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 1.        , ..., 0.16595852, 0.        ,\n",
       "        0.12169042],\n",
       "       ...,\n",
       "       [0.        , 0.        , 4.        , ..., 0.09134875, 0.        ,\n",
       "        0.05023666],\n",
       "       [0.        , 0.        , 4.        , ..., 0.48112811, 0.07308761,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 4.        , ..., 0.17415049, 0.25132248,\n",
       "        0.19154588]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 893us/step - accuracy: 0.6839 - loss: 0.6338 - val_accuracy: 0.8845 - val_loss: 0.3174\n",
      "Epoch 2/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 832us/step - accuracy: 0.9046 - loss: 0.3336 - val_accuracy: 0.9023 - val_loss: 0.3067\n",
      "Epoch 3/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 885us/step - accuracy: 0.9079 - loss: 0.3153 - val_accuracy: 0.9023 - val_loss: 0.3026\n",
      "Epoch 4/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 838us/step - accuracy: 0.9067 - loss: 0.3153 - val_accuracy: 0.9023 - val_loss: 0.3046\n",
      "Epoch 5/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 850us/step - accuracy: 0.9086 - loss: 0.3087 - val_accuracy: 0.9015 - val_loss: 0.3010\n",
      "Epoch 6/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 832us/step - accuracy: 0.9095 - loss: 0.3065 - val_accuracy: 0.8934 - val_loss: 0.3139\n",
      "Epoch 7/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 836us/step - accuracy: 0.9068 - loss: 0.3107 - val_accuracy: 0.9027 - val_loss: 0.3036\n",
      "Epoch 8/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 866us/step - accuracy: 0.9069 - loss: 0.3092 - val_accuracy: 0.9023 - val_loss: 0.3048\n",
      "Epoch 9/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 823us/step - accuracy: 0.9089 - loss: 0.3043 - val_accuracy: 0.9023 - val_loss: 0.3025\n",
      "Epoch 10/10\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 840us/step - accuracy: 0.9093 - loss: 0.3059 - val_accuracy: 0.9027 - val_loss: 0.3010\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - accuracy: 0.9014 - loss: 0.3023\n",
      "Test loss: 0.301032155752182\n",
      "Test accuracy: 0.9026656150817871\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(f\"Test loss: {scores[0]}\")\n",
    "print(f\"Test accuracy: {scores[1]}\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_nueral = model.predict(X_test)\n",
    "# y_pred_nueral = (y_pred_nueral > 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.8641    0.9556    0.9076      1238\n",
      "1             1    0.9503    0.8498    0.8972      1238\n",
      "2      accuracy                        0.9027      2476\n",
      "3     macro avg    0.9072    0.9027    0.9024      2476\n",
      "4  weighted avg    0.9072    0.9027    0.9024      2476\n",
      "5       overall  0.950316  0.849758  0.897228  0.902666\n"
     ]
    }
   ],
   "source": [
    "show_result(Y_test, y_pred_nueral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from keras.models import save_model, load_model\n",
    "\n",
    "# Save the model\n",
    "model.save('keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92766005],\n",
       "       [0.13133308],\n",
       "       [0.92488813],\n",
       "       ...,\n",
       "       [0.17713252],\n",
       "       [0.17160837],\n",
       "       [0.1896137 ]], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_nueral"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
