{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apps = pd.read_csv('../apps.tsv', delimiter='\\t',encoding='utf-8')\n",
    "# user_history = pd.read_csv('../user_history.tsv', delimiter='\\t',encoding='utf-8')\n",
    "# jobs = pd.read_csv('../input_data/jobs.tsv', delimiter='\\t',encoding='utf-8', on_bad_lines=\"skip\")\n",
    "# users = pd.read_csv('../input_data/users.tsv' ,delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "apps = pd.read_csv('apps.csv')\n",
    "jobs = pd.read_csv('jobs.csv')\n",
    "users = pd.read_csv('users.csv')\n",
    "user_history = pd.read_csv('work_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = users[users.Split==\"Train\"]\n",
    "test_user = users[users.Split==\"Test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "job_vectorizer = joblib.load('../Backend Algos/tfidf_data/job_description_tfidf.pkl')\n",
    "jobdesc_tfidf_matrix = joblib.load('../Backend Algos/tfidf_data/job_description_tfidf_matrix.pkl')\n",
    "\n",
    "work_history_vectorizer = joblib.load('../Backend Algos/tfidf_data/work_history_tfidf.pkl')\n",
    "\n",
    "# For USER USER Matching\n",
    "user_tfidf_matrix = joblib.load('../Backend Algos/tfidf_data/user_tfidf_matrix.pkl')\n",
    "user_tfidf_vectorizer = joblib.load('../Backend Algos/tfidf_data/user_tfidf_vectorizer.pkl')\n",
    "\n",
    "model = load_model('../Backend Algos/tfidf_data/keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<115684x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3494490 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobdesc_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115684, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopJobs(userIds):\n",
    "    # print(userIds)\n",
    "    jobsSet = set()\n",
    "    for user in userIds:\n",
    "        jobsAppliedTo = apps[apps['UserID']==user]\n",
    "        jobsAppliedTo = jobsAppliedTo['JobID'].values\n",
    "        for job in jobsAppliedTo:\n",
    "            jobsSet.add(job)\n",
    "            if (len(jobsSet) > 100):\n",
    "                break\n",
    "        if (len(jobsSet) > 100):\n",
    "                break\n",
    "    \n",
    "    return jobsSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buildXtrain(jobSet, userProfile, past_work_ex, city, state):\n",
    "    # Firstly, we create the user_profile to training_data form\n",
    "    user_feature = np.array(userProfile)\n",
    "    work_ex_transform = work_history_vectorizer.transform([past_work_ex])\n",
    "    user_feature = np.concatenate((user_feature, work_ex_transform.toarray()[0]))\n",
    "\n",
    "    X = np.zeros((1,158))\n",
    "\n",
    "    # Then, we add the job_transform_tfidf to the user\n",
    "    for jobId in jobSet:\n",
    "        jobInfo = jobs[jobs['JobID'] == jobId]\n",
    "        idx = jobInfo.index.values[0]\n",
    "        # print(jobdesc_tfidf_matrix.shape)\n",
    "        feature = np.concatenate((user_feature, jobdesc_tfidf_matrix[idx, :].toarray()[0]))\n",
    "\n",
    "        if jobInfo['City'].values[0] == city:\n",
    "            feature = np.append(feature, [1])\n",
    "        else:\n",
    "            feature = np.append(feature, [0])\n",
    "\n",
    "        if jobInfo['State'].values[0] == state:\n",
    "            feature = np.append(feature, [1])\n",
    "        else:\n",
    "            feature = np.append(feature, [0])\n",
    "\n",
    "        feature = feature.reshape(1,158)\n",
    "        X = np.concatenate((X, feature), axis=0)\n",
    "\n",
    "    # Finally, we rank all of them based on the probabilities of model prediction\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(input_data, N):\n",
    "    input_data['currentlyEmployed'] = 1 if input_data['currentlyEmployed'] == 'Yes' else 0\n",
    "    input_data['managedOthers'] = 1 if input_data['managedOthers'] == 'Yes' else 0\n",
    "    degree_mapping = {\n",
    "        'None': 0,\n",
    "        'High School': 1,\n",
    "        'Vocational': 2,\n",
    "        'Associate\\'s': 3,\n",
    "        'Bachelor\\'s': 4,\n",
    "        'Master\\'s': 5,\n",
    "        'PhD': 6\n",
    "    }\n",
    "    degree = input_data['degree']\n",
    "    input_data['degree'] = degree_mapping.get(input_data['degree'], 0)  \n",
    "\n",
    "    input_data_list = [input_data[field] for field in ['degree', 'workHistoryCount', \n",
    "                                                        'yearsOfExp', 'currentlyEmployed',\n",
    "                                                        'managedOthers', 'managedHowMany']]\n",
    "    \n",
    "    input_data_tf_idf_degree = degree + ' ' + input_data['major'] + ' ' + str(input_data['yearsOfExp'])\n",
    "    input_data_transformed = user_tfidf_vectorizer.transform([input_data_tf_idf_degree])\n",
    "\n",
    "    cosine_similarities = cosine_similarity(input_data_transformed, user_tfidf_matrix)\n",
    "    top_similar_users_indices = cosine_similarities.flatten().argsort()[::-1][:10]\n",
    "    most_similar_user = users.iloc[top_similar_users_indices]\n",
    "\n",
    "    # Get the top 100 jobs that similar users have applied in\n",
    "    top_jobs = getTopJobs(most_similar_user['UserID'].values)\n",
    "\n",
    "    # Now, re-rank the above 100 jobs and recommend the Top 20\n",
    "    Xtrain = buildXtrain(top_jobs, input_data_list, input_data['workHistory'], input_data['city'], input_data['state'])\n",
    "\n",
    "    prediction = model.predict(Xtrain, verbose=0)\n",
    "\n",
    "    top_jobs_list = list(top_jobs)\n",
    "    job_predictions = {}\n",
    "\n",
    "    # Iterate over each job ID in top_jobs and corresponding prediction value\n",
    "    for i, job_id in enumerate(top_jobs_list):\n",
    "        prediction_value = prediction[i][0]\n",
    "        job_predictions[job_id] = prediction_value\n",
    "    \n",
    "    sorted_job_predictions = dict(sorted(job_predictions.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    recommended_jobs = []\n",
    "    for job in sorted_job_predictions:\n",
    "        recommended_jobs.append(job)\n",
    "        if (len(recommended_jobs) >= N):\n",
    "            break\n",
    "    \n",
    "    return recommended_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 552/552 [00:27<00:00, 20.24it/s]\n"
     ]
    }
   ],
   "source": [
    "hit = []\n",
    "\n",
    "for _, user in tqdm(test_user.iterrows(), total=len(test_user)):\n",
    "    input_data = dict()\n",
    "    input_data['major'] = user['Major']\n",
    "    input_data['degree'] = user['DegreeType']\n",
    "\n",
    "    input_data['workHistoryCount'] = user['WorkHistoryCount']\n",
    "    input_data['managedHowMany'] = user['ManagedHowMany']\n",
    "    input_data['yearsOfExp'] = user['TotalYearsExperience']\n",
    "\n",
    "    input_data['currentlyEmployed'] = user['CurrentlyEmployed']\n",
    "    input_data['managedOthers'] = user['ManagedOthers']\n",
    "    input_data['city'] = user['City']\n",
    "    input_data['state'] = user['State']\n",
    "\n",
    "    id = user['UserID']\n",
    "    # print(id)\n",
    "    work_hist = user_history[user_history['UserID'] == id]\n",
    "    work_hist_concat = \"\"\n",
    "\n",
    "    for hist in work_hist.iterrows():\n",
    "        work_hist_concat += (hist[1]['JobTitle']) + \" \"\n",
    "\n",
    "    input_data['workHistory'] = work_hist_concat\n",
    "\n",
    "    predictions = getPredictions(input_data, 20)\n",
    "\n",
    "    found_applied = False\n",
    "    for job in predictions:\n",
    "        applied_to = apps[(apps['JobID'] == job) & (apps['UserID'] == id)]\n",
    "        if (len(applied_to) > 0):\n",
    "            found_applied = True\n",
    "            break\n",
    "    \n",
    "    if found_applied:\n",
    "        hit.append(1)\n",
    "    else:\n",
    "        hit.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5797101449275363"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
