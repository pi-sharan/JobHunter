{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = pd.read_csv('filtered_apps.csv')\n",
    "users = pd.read_csv('filtered_users.csv')\n",
    "jobs = pd.read_csv('filtered_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate dictionaries for mapping old id to new id for users and movies\n",
    "unique_JobID = apps['JobID'].unique()\n",
    "unique_UserID = apps['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "user_new2old_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    user_new2old_id_dict[j] = u\n",
    "    j += 1\n",
    "j = 0\n",
    "job_old2new_id_dict = dict()\n",
    "job_new2old_id_dict = dict()\n",
    "for i in unique_JobID:\n",
    "    job_old2new_id_dict[i] = j\n",
    "    job_new2old_id_dict[j] = i\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Then, use the generated dictionaries to reindex UserID and JobID in the data_df\n",
    "user_list = apps['UserID'].values\n",
    "job_list = apps['JobID'].values\n",
    "for j in range(len(apps)):\n",
    "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
    "    job_list[j] = job_old2new_id_dict[job_list[j]]\n",
    "apps['UserID'] = user_list\n",
    "apps['JobID'] = job_list\n",
    "\n",
    "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
    "train_index = np.random.random(len(apps)) <= 0.7\n",
    "train_df = apps[train_index]\n",
    "test_df = apps[~train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/qfrhvmb90676tpsym70kql640000gn/T/ipykernel_17602/3800284669.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['Applied?'] = 1\n",
      "/var/folders/cj/qfrhvmb90676tpsym70kql640000gn/T/ipykernel_17602/3800284669.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['Applied?'] = 1\n"
     ]
    }
   ],
   "source": [
    "train_df['Applied?'] = 1\n",
    "test_df['Applied?'] = 1\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# generate train_mat and test_mat\n",
    "num_users = len(apps['UserID'].unique())\n",
    "num_jobs = len(apps['JobID'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['Applied?'].values, (train_df['UserID'].values, train_df['JobID'].values)), shape=(num_users, num_jobs)).astype(float).toarray()\n",
    "test_mat = coo_matrix((test_df['Applied?'].values, (test_df['UserID'].values, test_df['JobID'].values)), shape=(num_users, num_jobs)).astype(float).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF:\n",
    "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
    "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
    "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
    "\n",
    "        self.latent = latent  # the latent dimension\n",
    "        self.lr = lr  # learning rate\n",
    "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
    "\n",
    "        self.num_users, self.num_jobs = train_mat.shape\n",
    "\n",
    "        self.sample_user, self.sample_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
    "        self.num_sample = len(self.sample_user)  # the number of user-movie pairs having ratings in train_mat\n",
    "\n",
    "        self.train_indicator_mat = 1.0 * (train_mat > 0)  # binary matrix to indicate whether s user-movie pair has rating or not in train_mat\n",
    "        self.test_indicator_mat = 1.0 * (test_mat > 0)  # binary matrix to indicate whether s user-movie pair has rating or not in test_mat\n",
    "\n",
    "        self.P = np.random.random((self.num_users, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
    "        self.Q = np.random.random((self.num_jobs, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
    "\n",
    "    def train(self, epoch=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Goal: Write your code to train your matrix factorization model for epoch iterations in this function\n",
    "        Input: epoch -- the number of training epoch\n",
    "        Output: epoch_loss_list -- a list recording the training loss for each epoch\n",
    "                epoch_test_RMSE_list -- a list recording the testing RMSE after each training epoch\n",
    "        \"\"\"\n",
    "        epoch_loss_list = []\n",
    "        epoch_test_RMSE_list = []\n",
    "        for ep in range(epoch):\n",
    "            \"\"\"\n",
    "            Write your code here to implement the training process for one epoch,\n",
    "            and at the end of each epoch, print out the epoch number, the training loss after this epoch,\n",
    "            and the test RMSE after this epoch\n",
    "            \"\"\"\n",
    "            random_indices = np.random.permutation(self.num_sample)\n",
    "            total_loss = 0\n",
    "            for idx in random_indices:\n",
    "              u, i = self.sample_user[idx], self.sample_movie[idx]\n",
    "              pred_val = np.dot(self.P[u], self.Q[i])\n",
    "              error = pred_val - self.train_mat[u, i]\n",
    "              total_loss += error ** 2\n",
    "              self.P[u] -= self.lr * ((error * self.Q[i]) + self.reg * self.P[u])\n",
    "              self.Q[i] -= self.lr * ((error * self.P[u]) + self.reg * self.Q[i])\n",
    "\n",
    "            total_loss /= self.num_sample\n",
    "            epoch_loss_list.append(total_loss)\n",
    "\n",
    "            test_RMSE = self.calculate_RMSE(self.predict(), self.test_mat)\n",
    "            epoch_test_RMSE_list.append(test_RMSE)\n",
    "\n",
    "            if verbose:\n",
    "              print(f\"Epoch {ep + 1} -- Training Loss: {total_loss}, Test RMSE: {test_RMSE}\")\n",
    "\n",
    "            \"\"\"\n",
    "            End of your code for this function\n",
    "            \"\"\"\n",
    "        return epoch_loss_list, epoch_test_RMSE_list\n",
    "\n",
    "    def calculate_RMSE(self, prediction_mat, true_mat):\n",
    "      num_ratings = np.sum(true_mat > 0)\n",
    "      squared_error = np.sum(((prediction_mat - true_mat) ** 2) * (true_mat > 0))\n",
    "      mean_squared_error = squared_error / num_ratings\n",
    "      RMSE = np.sqrt(mean_squared_error)\n",
    "      return RMSE\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        prediction_mat = np.matmul(self.P, self.Q.T)\n",
    "        return prediction_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Training Loss: 0.23662562398973788, Test RMSE: 0.4335657701206561\n",
      "Epoch 2 -- Training Loss: 0.14329545165047228, Test RMSE: 0.3670825961854032\n",
      "Epoch 3 -- Training Loss: 0.1004611596728579, Test RMSE: 0.3251845093274829\n",
      "Epoch 4 -- Training Loss: 0.0763572394361383, Test RMSE: 0.2959021980559313\n",
      "Epoch 5 -- Training Loss: 0.06104943011296383, Test RMSE: 0.2740328609181345\n",
      "Epoch 6 -- Training Loss: 0.050530936417249005, Test RMSE: 0.25691724977189206\n",
      "Epoch 7 -- Training Loss: 0.04289622466483379, Test RMSE: 0.24306595875488884\n",
      "Epoch 8 -- Training Loss: 0.03713164150311581, Test RMSE: 0.2315813967357987\n",
      "Epoch 9 -- Training Loss: 0.032641177539423374, Test RMSE: 0.22187015194477505\n",
      "Epoch 10 -- Training Loss: 0.02905817406061118, Test RMSE: 0.21352687249012747\n",
      "Epoch 11 -- Training Loss: 0.026140749492292985, Test RMSE: 0.2062618794431641\n",
      "Epoch 12 -- Training Loss: 0.023724405242030257, Test RMSE: 0.19986247818904482\n",
      "Epoch 13 -- Training Loss: 0.02169369882338611, Test RMSE: 0.19417489150370712\n",
      "Epoch 14 -- Training Loss: 0.019964953827412323, Test RMSE: 0.18907515346916692\n",
      "Epoch 15 -- Training Loss: 0.01847661554374038, Test RMSE: 0.1844689942036399\n",
      "Epoch 16 -- Training Loss: 0.017182555310039167, Test RMSE: 0.1802781012965694\n",
      "Epoch 17 -- Training Loss: 0.016048021951517867, Test RMSE: 0.17644128320954922\n",
      "Epoch 18 -- Training Loss: 0.015045474641366817, Test RMSE: 0.1729115954542824\n",
      "Epoch 19 -- Training Loss: 0.014153268400486807, Test RMSE: 0.16965571987671488\n",
      "Epoch 20 -- Training Loss: 0.013354081672023507, Test RMSE: 0.16663644591160892\n",
      "Epoch 21 -- Training Loss: 0.012634004011281244, Test RMSE: 0.1638224580341302\n",
      "Epoch 22 -- Training Loss: 0.011982351365367896, Test RMSE: 0.16119177201803075\n",
      "Epoch 23 -- Training Loss: 0.011389226786755653, Test RMSE: 0.158726871052124\n",
      "Epoch 24 -- Training Loss: 0.010847470972651893, Test RMSE: 0.1564118499659178\n",
      "Epoch 25 -- Training Loss: 0.010350513512313518, Test RMSE: 0.15422881951387912\n",
      "Epoch 26 -- Training Loss: 0.009893009042779588, Test RMSE: 0.1521672926614828\n",
      "Epoch 27 -- Training Loss: 0.009470552733350492, Test RMSE: 0.15021990824206427\n",
      "Epoch 28 -- Training Loss: 0.009078993350102251, Test RMSE: 0.14836747061097288\n",
      "Epoch 29 -- Training Loss: 0.008715224820260176, Test RMSE: 0.14660519185890722\n",
      "Epoch 30 -- Training Loss: 0.008376391862348492, Test RMSE: 0.14492636634389722\n"
     ]
    }
   ],
   "source": [
    "mf = MF(train_mat, test_mat, latent=5, lr=0.01, reg=0.001)\n",
    "epoch_loss_list, epoch_test_RMSE_list = mf.train(epoch=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSIS**\n",
    "\n",
    "Now, for a particular user, I will try and find out which jobs should be recommended to him the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mat = mf.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00534711, 1.04179893, 1.10880591, ..., 1.43660337, 0.83723732,\n",
       "        1.275175  ],\n",
       "       [1.19520614, 0.81565988, 0.77423111, ..., 1.24661053, 1.39388445,\n",
       "        1.20707195],\n",
       "       [1.16666736, 1.07543694, 1.00729028, ..., 1.40799141, 1.05904334,\n",
       "        1.42275828],\n",
       "       ...,\n",
       "       [1.24300291, 0.99170949, 1.15150167, ..., 1.51532976, 1.07380632,\n",
       "        1.08500007],\n",
       "       [1.46015168, 1.24346454, 1.38321907, ..., 1.84409969, 1.25558939,\n",
       "        1.44928774],\n",
       "       [1.51998836, 1.23179488, 1.383776  , ..., 1.81766253, 1.14899964,\n",
       "        1.32001119]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_recommendation_for_user(user_id):\n",
    "    arr = pred_mat[user_id]\n",
    "    indexed_arr = list(enumerate(arr))\n",
    "\n",
    "    # Sort the array of tuples based on probabilities in descending order\n",
    "    sorted_arr = sorted(indexed_arr, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract the indices of the top 10 probabilities\n",
    "    top_10_indices = [job_new2old_id_dict[index] for index, _ in sorted_arr[:10]]\n",
    "\n",
    "    best_jobs = jobs[jobs['JobID'].isin(top_10_indices)]\n",
    "    print(\"Top 10 jobs for the user are: \\n\")\n",
    "    \n",
    "    for _, job in best_jobs.iterrows():\n",
    "        print(job['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 jobs for the user are: \n",
      "\n",
      "Material Handler â€“ PR1953\n",
      "Medical Assistant for Pediatric Cardiology\n",
      "Administrative Assistant\n",
      "CUSTOMER SERVICE REPS\n",
      "Warehouse / Driver\n",
      "Accounting Payroll\n",
      "Administrative Assistant\n",
      "Contract Administrative Assistant\n",
      "Assembly Technician\n",
      "Accounting/Financial Recruiter\n"
     ]
    }
   ],
   "source": [
    "find_best_recommendation_for_user(543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
