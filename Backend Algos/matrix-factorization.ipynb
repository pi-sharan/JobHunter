{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = pd.read_csv('filtered_apps.csv')\n",
    "users = pd.read_csv('filtered_users.csv')\n",
    "jobs = pd.read_csv('filtered_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate dictionaries for mapping old id to new id for users and movies\n",
    "unique_JobID = apps['JobID'].unique()\n",
    "unique_UserID = apps['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "user_new2old_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    user_new2old_id_dict[j] = u\n",
    "    j += 1\n",
    "j = 0\n",
    "job_old2new_id_dict = dict()\n",
    "job_new2old_id_dict = dict()\n",
    "for i in unique_JobID:\n",
    "    job_old2new_id_dict[i] = j\n",
    "    job_new2old_id_dict[j] = i\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Then, use the generated dictionaries to reindex UserID and JobID in the data_df\n",
    "user_list = apps['UserID'].values\n",
    "job_list = apps['JobID'].values\n",
    "for j in range(len(apps)):\n",
    "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
    "    job_list[j] = job_old2new_id_dict[job_list[j]]\n",
    "apps['UserID'] = user_list\n",
    "apps['JobID'] = job_list\n",
    "\n",
    "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
    "train_index = np.random.random(len(apps)) <= 0.7\n",
    "train_df = apps[train_index]\n",
    "test_df = apps[~train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/qfrhvmb90676tpsym70kql640000gn/T/ipykernel_11191/3800284669.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['Applied?'] = 1\n",
      "/var/folders/cj/qfrhvmb90676tpsym70kql640000gn/T/ipykernel_11191/3800284669.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['Applied?'] = 1\n"
     ]
    }
   ],
   "source": [
    "train_df['Applied?'] = 1\n",
    "test_df['Applied?'] = 1\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# generate train_mat and test_mat\n",
    "num_users = len(apps['UserID'].unique())\n",
    "num_jobs = len(apps['JobID'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['Applied?'].values, (train_df['UserID'].values, train_df['JobID'].values)), shape=(num_users, num_jobs)).astype(float).toarray()\n",
    "test_mat = coo_matrix((test_df['Applied?'].values, (test_df['UserID'].values, test_df['JobID'].values)), shape=(num_users, num_jobs)).astype(float).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF:\n",
    "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
    "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
    "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
    "\n",
    "        self.latent = latent  # the latent dimension\n",
    "        self.lr = lr  # learning rate\n",
    "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
    "\n",
    "        self.num_users, self.num_jobs = train_mat.shape\n",
    "\n",
    "        self.sample_user, self.sample_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
    "        self.num_sample = len(self.sample_user)  # the number of user-movie pairs having ratings in train_mat\n",
    "\n",
    "        self.train_indicator_mat = 1.0 * (train_mat > 0)  # binary matrix to indicate whether s user-movie pair has rating or not in train_mat\n",
    "        self.test_indicator_mat = 1.0 * (test_mat > 0)  # binary matrix to indicate whether s user-movie pair has rating or not in test_mat\n",
    "\n",
    "        self.P = np.random.random((self.num_users, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
    "        self.Q = np.random.random((self.num_jobs, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
    "\n",
    "    def train(self, epoch=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Goal: Write your code to train your matrix factorization model for epoch iterations in this function\n",
    "        Input: epoch -- the number of training epoch\n",
    "        Output: epoch_loss_list -- a list recording the training loss for each epoch\n",
    "                epoch_test_RMSE_list -- a list recording the testing RMSE after each training epoch\n",
    "        \"\"\"\n",
    "        epoch_loss_list = []\n",
    "        epoch_test_RMSE_list = []\n",
    "        for ep in range(epoch):\n",
    "            \"\"\"\n",
    "            Write your code here to implement the training process for one epoch,\n",
    "            and at the end of each epoch, print out the epoch number, the training loss after this epoch,\n",
    "            and the test RMSE after this epoch\n",
    "            \"\"\"\n",
    "            random_indices = np.random.permutation(self.num_sample)\n",
    "            total_loss = 0\n",
    "            for idx in random_indices:\n",
    "              u, i = self.sample_user[idx], self.sample_movie[idx]\n",
    "              pred_val = np.dot(self.P[u], self.Q[i])\n",
    "              error = pred_val - self.train_mat[u, i]\n",
    "              total_loss += error ** 2\n",
    "              self.P[u] -= self.lr * ((error * self.Q[i]) + self.reg * self.P[u])\n",
    "              self.Q[i] -= self.lr * ((error * self.P[u]) + self.reg * self.Q[i])\n",
    "\n",
    "            total_loss /= self.num_sample\n",
    "            epoch_loss_list.append(total_loss)\n",
    "\n",
    "            test_RMSE = self.calculate_RMSE(self.predict(), self.test_mat)\n",
    "            epoch_test_RMSE_list.append(test_RMSE)\n",
    "\n",
    "            if verbose:\n",
    "              print(f\"Epoch {ep + 1} -- Training Loss: {total_loss}, Test RMSE: {test_RMSE}\")\n",
    "\n",
    "            \"\"\"\n",
    "            End of your code for this function\n",
    "            \"\"\"\n",
    "        return epoch_loss_list, epoch_test_RMSE_list\n",
    "\n",
    "    def calculate_RMSE(self, prediction_mat, true_mat):\n",
    "      num_ratings = np.sum(true_mat > 0)\n",
    "      squared_error = np.sum(((prediction_mat - true_mat) ** 2) * (true_mat > 0))\n",
    "      mean_squared_error = squared_error / num_ratings\n",
    "      RMSE = np.sqrt(mean_squared_error)\n",
    "      return RMSE\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        prediction_mat = np.matmul(self.P, self.Q.T)\n",
    "        return prediction_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Training Loss: 0.23880399981489694, Test RMSE: 0.43340931719288217\n",
      "Epoch 2 -- Training Loss: 0.14440632617864038, Test RMSE: 0.36581912237548186\n",
      "Epoch 3 -- Training Loss: 0.10112848125100289, Test RMSE: 0.32327179500198466\n",
      "Epoch 4 -- Training Loss: 0.07682663233892618, Test RMSE: 0.29357428832590204\n",
      "Epoch 5 -- Training Loss: 0.061423573370473786, Test RMSE: 0.27143969266475504\n",
      "Epoch 6 -- Training Loss: 0.05086008476695606, Test RMSE: 0.2541520539667284\n",
      "Epoch 7 -- Training Loss: 0.0432039779533634, Test RMSE: 0.24020993627409815\n",
      "Epoch 8 -- Training Loss: 0.03742750993782589, Test RMSE: 0.2286893154854827\n",
      "Epoch 9 -- Training Loss: 0.03293223536749651, Test RMSE: 0.21898768736247134\n",
      "Epoch 10 -- Training Loss: 0.029345643065866064, Test RMSE: 0.21067466965652656\n",
      "Epoch 11 -- Training Loss: 0.026425792289252913, Test RMSE: 0.2034634103678997\n",
      "Epoch 12 -- Training Loss: 0.02400752731507474, Test RMSE: 0.19713776217219164\n",
      "Epoch 13 -- Training Loss: 0.021973887403980864, Test RMSE: 0.19152275036215083\n",
      "Epoch 14 -- Training Loss: 0.020241881714694036, Test RMSE: 0.18650087677403868\n",
      "Epoch 15 -- Training Loss: 0.018750094659159507, Test RMSE: 0.18197571180448713\n",
      "Epoch 16 -- Training Loss: 0.017452692470840334, Test RMSE: 0.17786932442799244\n",
      "Epoch 17 -- Training Loss: 0.01631387547313603, Test RMSE: 0.174115168951713\n",
      "Epoch 18 -- Training Loss: 0.015306661070057974, Test RMSE: 0.17067103537426884\n",
      "Epoch 19 -- Training Loss: 0.014409395540045085, Test RMSE: 0.16749473844678003\n",
      "Epoch 20 -- Training Loss: 0.013605404190036993, Test RMSE: 0.16454937670506806\n",
      "Epoch 21 -- Training Loss: 0.012880053175623143, Test RMSE: 0.16181370991879754\n",
      "Epoch 22 -- Training Loss: 0.01222263220647969, Test RMSE: 0.15924827407027284\n",
      "Epoch 23 -- Training Loss: 0.011624485792691461, Test RMSE: 0.15685176872251091\n",
      "Epoch 24 -- Training Loss: 0.011077133296424463, Test RMSE: 0.15460338827104902\n",
      "Epoch 25 -- Training Loss: 0.010574645464345488, Test RMSE: 0.15247955175474526\n",
      "Epoch 26 -- Training Loss: 0.010111656795186857, Test RMSE: 0.1504730779710693\n",
      "Epoch 27 -- Training Loss: 0.009683809689756984, Test RMSE: 0.14857435643453826\n",
      "Epoch 28 -- Training Loss: 0.009286835675977429, Test RMSE: 0.14677335296102298\n",
      "Epoch 29 -- Training Loss: 0.008917847550737585, Test RMSE: 0.14506321352385743\n",
      "Epoch 30 -- Training Loss: 0.008573652357225786, Test RMSE: 0.14343073784904548\n",
      "Epoch 31 -- Training Loss: 0.008252204467401877, Test RMSE: 0.1418755818776056\n",
      "Epoch 32 -- Training Loss: 0.007950983378621044, Test RMSE: 0.1403876223272076\n",
      "Epoch 33 -- Training Loss: 0.00766818086240583, Test RMSE: 0.13896433880307038\n",
      "Epoch 34 -- Training Loss: 0.007402384030546118, Test RMSE: 0.1375986492969119\n",
      "Epoch 35 -- Training Loss: 0.007151966059506211, Test RMSE: 0.13629062273860154\n",
      "Epoch 36 -- Training Loss: 0.006915345087717216, Test RMSE: 0.13502855185376195\n",
      "Epoch 37 -- Training Loss: 0.00669226517079394, Test RMSE: 0.13381919509490958\n",
      "Epoch 38 -- Training Loss: 0.006480614971104724, Test RMSE: 0.13265296670163476\n",
      "Epoch 39 -- Training Loss: 0.00628009546466721, Test RMSE: 0.1315305539935729\n",
      "Epoch 40 -- Training Loss: 0.006089807025201624, Test RMSE: 0.13044664897864933\n",
      "Epoch 41 -- Training Loss: 0.005908854196240321, Test RMSE: 0.1294020009185435\n",
      "Epoch 42 -- Training Loss: 0.0057367742090628955, Test RMSE: 0.12839310830671405\n",
      "Epoch 43 -- Training Loss: 0.005572691384790936, Test RMSE: 0.12741784912567436\n",
      "Epoch 44 -- Training Loss: 0.005416407924247095, Test RMSE: 0.12647400186850025\n",
      "Epoch 45 -- Training Loss: 0.005267093600073305, Test RMSE: 0.12555783270577323\n",
      "Epoch 46 -- Training Loss: 0.005124566721703675, Test RMSE: 0.12467096640670167\n",
      "Epoch 47 -- Training Loss: 0.004988086380383888, Test RMSE: 0.1238091391859729\n",
      "Epoch 48 -- Training Loss: 0.004857552032849889, Test RMSE: 0.12297821131406733\n",
      "Epoch 49 -- Training Loss: 0.004732457965808812, Test RMSE: 0.12216851108335436\n",
      "Epoch 50 -- Training Loss: 0.004612458790700751, Test RMSE: 0.12138089492356718\n"
     ]
    }
   ],
   "source": [
    "mf = MF(train_mat, test_mat, latent=5, lr=0.01, reg=0.001)\n",
    "epoch_loss_list, epoch_test_RMSE_list = mf.train(epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSIS**\n",
    "\n",
    "Now, for a particular user, I will try and find out which jobs should be recommended to him the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mat = mf.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_recommendation_for_user(user_id):\n",
    "    arr = pred_mat[user_id]\n",
    "    indexed_arr = list(enumerate(arr))\n",
    "\n",
    "    # Sort the array of tuples based on probabilities in descending order\n",
    "    sorted_arr = sorted(indexed_arr, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract the indices of the top 10 probabilities\n",
    "    top_10_indices = [job_new2old_id_dict[index] for index, _ in sorted_arr[:10]]\n",
    "\n",
    "    best_jobs = jobs[jobs['JobID'].isin(top_10_indices)]\n",
    "\n",
    "    print(\"Top 10 jobs for the user are: \\n\")\n",
    "    \n",
    "    for _, job in best_jobs.iterrows():\n",
    "        print(job['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 jobs for the user are: \n",
      "\n",
      "Administrative Assistant\n",
      "Data Entry Clerk\n",
      "Front Office Receptionist\n",
      "ADMINISTRATIVE ASSISTANT-CHILD PROTECTION SERVICES\n",
      "Scheduler / Staffing Coordinator / Office\n",
      "RECEPTIONIST\n",
      "Customer Service & Sales $10.50-12.00\n",
      "Customer Service Representatives Needed ASAP!\n",
      "Electronic Equipment Assemblers\n",
      "Solidworks or Pro-E Checker True Position  GD&T $55 an hour\n"
     ]
    }
   ],
   "source": [
    "find_best_recommendation_for_user(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
